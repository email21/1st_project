import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import pickle
import logging
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import os

class VectorStore:  
    def __init__(self, model_name: str = "kakaobank/kf-deberta-base"):
        self.model_name: str = model_name
        self.encoder = SentenceTransformer(model_name)
        self.index: Optional[faiss.Index] = None
        self.documents: List[Dict] = []
        self.dimension: int = 768  # KF-DeBERTaì˜ ê¸°ë³¸ hidden_size
        self._setup_logging()
    
    def _setup_logging(self) -> None:       
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
     
        self.logger: logging.Logger = logging.getLogger(__name__)
        self.logger.info(f"ğŸ¤– KF-DeBERTa ëª¨ë¸ ({self.model_name}) ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_processed_data(self, data_dir: str = "../data/processed") -> None:
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
        self.logger.info("ğŸ“ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì‹œì‘")
        
        product_files: Dict[str, str] = {
            'deposit': 'deposit_processed.csv',
            'saving': 'saving_processed.csv',
            'annuity': 'annuity_processed.csv',
            'mortgage': 'mortgage_processed.csv',
            'rent': 'rent_processed.csv',
            'credit': 'credit_processed.csv'
        }
                
        for product_type, filename in product_files.items():
            filepath = os.path.join(data_dir, filename)
            if os.path.exists(filepath):
                try:
                    df = pd.read_csv(filepath)
                    for _, row in df.iterrows():
                        doc = {
                            'product_type': product_type,
                            'company': row.get('kor_co_nm', ''),
                            'product_name': row.get('fin_prdt_nm', ''),
                            'search_text': row.get('search_text', ''),
                            'raw_data': row.to_dict()
                        }
                        self.documents.append(doc)
                    
                    self.logger.info(f"âœ… {product_type}: {len(df)}ê°œ ìƒí’ˆ ë¡œë“œ")
                except Exception as e:
                    self.logger.error(f"âŒ {product_type} ë¡œë“œ ì‹¤íŒ¨: {e}")
            else:
                self.logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {filepath}")
        
        self.logger.info(f"ğŸ“Š ì´ {len(self.documents)}ê°œ ìƒí’ˆ ë¡œë“œ ì™„ë£Œ")
    
    def build_index(self) -> None:
        """FAISS ì¸ë±ìŠ¤ êµ¬ì¶• """
        if not self.documents:
            self.logger.error("âŒ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”.")
            return
        
        self.logger.info("ğŸ”§ KF-DeBERTa ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘")
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (KF-DeBERTa ì‚¬ìš©)
        texts = [doc['search_text'] for doc in self.documents]
        
        # KF-DeBERTaë¡œ ì„ë² ë”© ìƒì„±
        self.logger.info("ğŸ§  KF-DeBERTaë¡œ ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = self.encoder.encode(
            texts, 
            show_progress_bar=True,
            batch_size=32,  # ë°°ì¹˜ í¬ê¸° ì„¤ì •
            convert_to_numpy=True
        )
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        self.index = faiss.IndexFlatIP(self.dimension)
        
        # ì •ê·œí™” í›„ ì¸ë±ìŠ¤ ì¶”ê°€
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings.astype('float32'))
        
        self.logger.info(f"âœ… KF-DeBERTa ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
    
    def search(self, query: str, top_k: int = 5, product_type: Optional[str] = None) -> List[Dict]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰ """
        if self.index is None:
            self.logger.error("âŒ ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”© 
        query_embedding = self.encoder.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(query_embedding)
        
        # ê²€ìƒ‰ ì‹¤í–‰
        scores, indices = self.index.search(query_embedding.astype('float32'), top_k * 2)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.documents):
                doc = self.documents[idx].copy()
                doc['similarity_score'] = float(score)
                
                # ìƒí’ˆ íƒ€ì… í•„í„°ë§
                if product_type is None or doc['product_type'] == product_type:
                    results.append(doc)
                    
                if len(results) >= top_k:
                    break
        
        return results
    
    def get_embedding(self, text: str) -> np.ndarray:
        """KF-DeBERTaë¡œ ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        embedding = self.encoder.encode([text], convert_to_numpy=True)
        return embedding[0]
    
    def batch_search(self, queries: List[str], top_k: int = 5) -> List[List[Dict]]:
        """ë°°ì¹˜ ê²€ìƒ‰ - ì—¬ëŸ¬ ì¿¼ë¦¬ ë™ì‹œ ì²˜ë¦¬"""
        if self.index is None:
            self.logger.error("âŒ ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
        query_embeddings = self.encoder.encode(queries, convert_to_numpy=True)
        faiss.normalize_L2(query_embeddings)
        
        # ë°°ì¹˜ ê²€ìƒ‰
        scores, indices = self.index.search(query_embeddings.astype('float32'), top_k)
        
        batch_results = []
        for query_scores, query_indices in zip(scores, indices):
            query_results = []
            for score, idx in zip(query_scores, query_indices):
                if idx < len(self.documents):
                    doc = self.documents[idx].copy()
                    doc['similarity_score'] = float(score)
                    query_results.append(doc)
            batch_results.append(query_results)
        
        return batch_results
    
    def save_index(self, save_path: str = "../data/vector_store") -> None:

        try:
            save_path = os.path.abspath(save_path)
            os.makedirs(save_path, exist_ok=True)
            self.logger.info(f"ğŸ“ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
            self.logger.info(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {save_path}")
            
            if self.index is None:
                self.logger.error("âŒ ì¸ë±ìŠ¤ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
            
            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            index_file = os.path.join(save_path, "kf_deberta_index.bin")
            faiss.write_index(self.index, index_file)
            self.logger.info(f"âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥: {index_file}")
            
            # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥
            docs_file = os.path.join(save_path, "documents.pkl")
            with open(docs_file, 'wb') as f:
                pickle.dump(self.documents, f)
            self.logger.info(f"âœ… ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥: {docs_file}")
            
            # ëª¨ë¸ ì •ë³´ ì €ì¥
            info_file = os.path.join(save_path, "model_info.txt")
            with open(info_file, 'w', encoding='utf-8') as f:
                f.write(f"Model: {self.model_name}\n")
                f.write(f"Dimension: {self.dimension}\n")
                f.write(f"Documents: {len(self.documents)}\n")
            self.logger.info(f"âœ… ëª¨ë¸ ì •ë³´ ì €ì¥: {info_file}")
            
            saved_files = os.listdir(save_path)
            self.logger.info(f"ğŸ“‹ ì €ì¥ëœ íŒŒì¼ë“¤: {saved_files}")
            
        except Exception as e:
            self.logger.error(f"âŒ ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            print(f"ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_index(self, load_path: str = "../data/vector_store") -> bool:
        try:
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            index_file = os.path.join(load_path, "kf_deberta_index.bin")
            if os.path.exists(index_file):
                self.index = faiss.read_index(index_file)
            else:
                # ê¸°ì¡´ íŒŒì¼ëª…ë„ ì‹œë„
                self.index = faiss.read_index(os.path.join(load_path, "faiss_index.bin"))
            
            # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(os.path.join(load_path, "documents.pkl"), 'rb') as f:
                self.documents = pickle.load(f)
            
            self.logger.info(f"âœ… KF-DeBERTa ë²¡í„° ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def get_model_info(self) -> Dict:
        return {
            'model_name': self.model_name,
            'model_type': 'KF-DeBERTa',
            'dimension': self.dimension,
            'document_count': len(self.documents),
            'index_built': self.index is not None
        }

if __name__ == "__main__":
    print("ğŸš€ KF-DeBERTa ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì‹œì‘")
    vector_store = VectorStore(model_name="kakaobank/kf-deberta-base")
    
    info = vector_store.get_model_info()
    print(f"ğŸ“Š ëª¨ë¸ ì •ë³´: {info}")
    
    vector_store.load_processed_data()
    vector_store.build_index()
    vector_store.save_index()
    
    test_query = "ë†’ì€ ê¸ˆë¦¬ ì˜ˆê¸ˆ ìƒí’ˆ"
    results = vector_store.search(test_query, top_k=3)
    
    print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰: '{test_query}'")
    for i, result in enumerate(results):
        print(f"{i+1}. {result['product_name']} (ìœ ì‚¬ë„: {result['similarity_score']:.4f})")
    
    print("âœ… KF-DeBERTa ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ!")
